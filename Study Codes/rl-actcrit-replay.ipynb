{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE에서와 같이 **episodic reward**를 reward function으로 설정하자. 정책 $\\pi_\\theta$에서 reward function $J(\\theta)$는 다음과 같이 정의된다.\n",
    "\n",
    "$$ J(\\theta) = \\mathbb{E}_{S_0 \\sim d_0, \\pi_\\theta}\\left[ \\sum_{t=0}^{\\infty}{\\gamma^t R_{t+1}} \\right] = \\mathbb{E}_{S_0 \\sim d_0, \\pi_\\theta}\\left[ G_0 \\right] = \\mathbb{E}_{S_0 \\sim d_0}\\left[ v_{\\pi_\\theta} (S_0) \\right] $$\n",
    "\n",
    "Policy Gradient Theorem(episodic)에 의하여, 위 식의 그래디언트인 $\\nabla_\\theta J(\\theta)$ 는 다음과 같다.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\left. \\sum_{t=0}^{T}{\\gamma^t q_{\\pi_\\theta}(S_t, A_t) \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t)} \\right| S_0 \\sim d_0 \\right]$$\n",
    "\n",
    "또한, 수학적으로 다음과 같은 성질이 성립한다.\n",
    "\n",
    "$$ \\mathbb{E}_{\\pi_\\theta} \\left[ b \\cdot \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t) \\right] = 0 $$\n",
    "\n",
    "이 성질을 사용하면 단순히 $q_{\\pi_\\theta}(S_t, A_t)$ 를 reward로 사용하는 것이 아닌, 평균을 빼서 더 안정적인 reward를 계산할 수 있다. 그 평균값은 $v_{\\pi_\\theta}(S_t)$ 이며, 이를 반영한 gradient 식은 다음과 같다.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\left. \\sum_{t=0}^{T}{\\gamma^t (q_{\\pi_\\theta}(S_t, A_t) - v_{\\pi_\\theta}(S_t) ) \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t)} \\right| S_0 \\sim d_0 \\right]$$\n",
    "\n",
    "$q_{\\pi_\\theta}(S_t, A_t) = \\mathbb{E}_{\\pi_\\theta}\\left[ \\left. G_t \\right| S_t, A_t \\right]$ 이므로 지금과 같은 stochastic 조건 하에서 $q_{\\pi_\\theta}(S_t, A_t) \\approx G_t$ 로 간주할 수 있다. 다만 이 경우에서는 $v_{\\pi_\\theta}$ 함수를 사용할 수 있으므로 각 시행마다 발생하는 variance를 줄이기 위해 $G_t$ 대신 $G_t \\approx R_{t+1} + \\gamma v_{\\textbf{w}}(S_{t+1})$ 를 사용한다. 또한 일반적으로 $\\gamma^t$ 항은 생략한다.\n",
    "\n",
    "기존의 Monte-Carlo식의 REINFORCE와는 다르게, Actor-Critic에서 업데이트는 매 $t$ 마다 아래처럼 이루어진다.\n",
    "\n",
    "$$ \\delta_t = R_{t+1} + \\gamma v_{\\textbf{w}}(S_{t+1}) - v_{\\textbf{w}}(S_t) $$\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\beta \\cdot \\delta_t \\nabla_\\textbf{w} v_{\\textbf{w}}(S_t) $$\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha \\cdot \\delta_t \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlcooZdcE-h5"
   },
   "source": [
    "### 패키지 import\n",
    "의존성 있는 패키지인 ``` numpy ```, ``` tensorflow ```, ``` keras ```를 import해야 한다.  \n",
    "  \n",
    "참고: keras는 단독으로 import하지 않고 반드시 ``` import tensorflow.keras as keras ```와 같이 ``` tensorflow ```를 통해 import해야 잠재적인 오류를 예방할 수 있다.  \n",
    "\n",
    "주어진 모듈을 설치하기 위해서는 `pip install numpy`, `pip install tensorflow`를 실행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-hm_1nH0E-h6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_hP_xapE-h7"
   },
   "source": [
    "### ReplayBuffer 클래스\n",
    "Replay buffer란 이전의 (S, a, S', R) 순서쌍들을 모아 둔 거대한 테이블을 뜻한다. 강화학습 특성상 환경과 상호작용하기에 시간 순서대로 데이터가 생성되는 즉시 학습시키면 bias가 발생하기 쉽다. 이러한 buffer를 도입함으로써 문제를 해결할 수 있다.  \n",
    " \n",
    "이 class는 다음과 같은 method로 구성된다.\n",
    "* 생성자\n",
    "  * ```replay_size``` 변수에 최대 버퍼크기 명시\n",
    "  * ```input_dims```에 state feature의 dimension 명시\n",
    "* ```store```을 통해 S, a, S', R 값과 terminate 여부를 저장\n",
    "* ```sample```을 통해 주어진 개수만큼 random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "avPl1viwE-h7"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, replay_size, input_dims):\n",
    "        self.replay_size = replay_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.state = np.zeros((self.replay_size, *input_dims), dtype=np.float32)\n",
    "        self.next_state = np.zeros((self.replay_size, *input_dims), dtype=np.float32)\n",
    "        self.action = np.zeros(self.replay_size, dtype=np.int32)\n",
    "        self.reward = np.zeros(self.replay_size, dtype=np.float32)\n",
    "        self.terminate = np.zeros(self.replay_size, dtype=np.int32)\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, terminate):\n",
    "        ''' ReplayBuffer에 단일 데이터쌍 저장 '''\n",
    "        index = self.mem_counter % self.replay_size\n",
    "\n",
    "        self.state[index]       = state\n",
    "        self.next_state[index]  = next_state\n",
    "        self.action[index]      = action\n",
    "        self.reward[index]      = reward\n",
    "        self.terminate[index]   = terminate\n",
    "\n",
    "        self.mem_counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ''' ReplayBuffer에서 batch_size만큼 임의의 데이터 샘플링 '''\n",
    "\n",
    "        # 아직 replay buffer가 다 차지 않은 경우를 고려해 실제 space_size를 min함수를 이용해 산정\n",
    "        space_size = min(self.mem_counter, self.replay_size)\n",
    "        batch = np.random.choice(space_size, batch_size, replace=False)\n",
    "\n",
    "        state       = self.state[batch]\n",
    "        next_state  = self.next_state[batch]\n",
    "        action      = self.action[batch]\n",
    "        reward      = self.reward[batch]\n",
    "        terminate   = self.terminate[batch]\n",
    "\n",
    "        return state, action, reward, next_state, terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor & Critic Model Building\n",
    "* ``` l_rate ```는 학습률을 나타낸다.\n",
    "* ``` n_actions ```는 tabular action의 개수를 나타낸다.\n",
    "* ``` input_dims ```는 state의 feature dimension이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    # pi 함수\n",
    "    def __init__(self, l_rate, n_actions, input_dims):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.l_rate     = l_rate\n",
    "        self.n_actions  = n_actions\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.layer1 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer2 = keras.layers.Dense(128, activation='relu')\n",
    "        self.layer3 = keras.layers.Dense(n_actions, activation='softmax')\n",
    "\n",
    "        self.compile(optimizer=keras.optimizers.Adam(learning_rate=self.l_rate))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mid1 = self.layer1(inputs)\n",
    "        mid2 = self.layer2(mid1)\n",
    "        return self.layer3(mid2)\n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    # v 함수\n",
    "    def __init__(self, l_rate, n_actions, input_dims):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.l_rate     = l_rate\n",
    "        self.n_actions  = n_actions\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.layer1 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer2 = keras.layers.Dense(128, activation='relu')\n",
    "        self.layer3 = keras.layers.Dense(n_actions)\n",
    "\n",
    "        self.compile(optimizer=keras.optimizers.Adam(learning_rate=self.l_rate))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mid1 = self.layer1(inputs)\n",
    "        mid2 = self.layer2(mid1)\n",
    "        return self.layer3(mid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoricalSelect(L):\n",
    "    ''' L[i] 확률로 i를 반환하는 함수. 어떠한 i에도 해당하지 않으면 0 반환. 0 <= sum(L) <= 1인 경우에만 정의됨.'''\n",
    "    L = list(tf.squeeze(L))\n",
    "    pick = np.random.random()\n",
    "    cumul = 0\n",
    "\n",
    "    for i, pi in enumerate(L):\n",
    "        if (cumul <= pick < cumul + float(pi)):\n",
    "            return i\n",
    "        cumul += pi\n",
    "\n",
    "    return 0\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self,\n",
    "            input_dims, n_actions,          # 환경 정의\n",
    "            actor_rate, critic_rate, gamma, # 학습 하이퍼파라미터\n",
    "            replay_size, batch_size,        # ReplayBuffer 정의\n",
    "            filename):\n",
    "\n",
    "        self.read_only = False  # ActorCriticDeploy 클래스에서 True\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.action = [i for i in range(n_actions)]\n",
    "        self.policy = ActorNetwork(actor_rate, n_actions, input_dims)\n",
    "        self.value  = CriticNetwork(critic_rate, n_actions, input_dims)\n",
    "\n",
    "        self.memory = ReplayBuffer(replay_size, input_dims)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.filename = filename  # model 저장용\n",
    "\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        ''' internal ReplayBuffer에 상태 저장 '''\n",
    "        if self.read_only:\n",
    "            raise Exception(\"DQNDeploy class can only use model, not store.\")\n",
    "            return\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        ''' 내부 stochastic policy인 `self.policy`를 토대로 action 결정 '''\n",
    "        state = np.array([obs], dtype=np.float32) # 관찰 결과를 numpy 스타일로 변환 (tf.convert_to_tensor도 ok)\n",
    "        pr = self.policy(state) # 신경망으로 전달, 확률 pr에 저장\n",
    "        return categoricalSelect(pr)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.read_only:  # 읽기 전용\n",
    "            raise Exception(\"ActorCriticDeploy class can only use model, not learn.\")\n",
    "            return\n",
    "        if self.memory.mem_counter < self.batch_size:   # 충분한 데이터 수집 전까지 학습 x\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, done = \\\n",
    "            self.memory.sample(self.batch_size) # 데이터를 ReplayBuffer에서 샘플링\n",
    "        \n",
    "        # tf.GradientTape에서 사용 위한 전처리 과정\n",
    "        act_list = tf.cast(actions, dtype=tf.int32)             # action의 기록\n",
    "        episode = act_list.shape[0]\n",
    "        episode_list = tf.range(episode, dtype=tf.int32) # action이 일어났던 episode의 기록\n",
    "        act_list, episode_list = tf.reshape(act_list, (episode, 1)), tf.reshape(episode_list, (episode, 1))\n",
    "        zip_list = tf.concat((episode_list, act_list), axis=1)     # action & episode 텐서의 병합\n",
    "        gamma = tf.constant(self.gamma, dtype=tf.float32)\n",
    "        done = tf.cast(done, tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:  # Gradient를 policy/value에 2회 적용하므로 persistent\n",
    "            Pr    = self.policy(states)     # n x n, 이걸 n으로 만들어야 함.\n",
    "            Vthis = self.value(states)\n",
    "            Vnext = self.value(next_states)\n",
    "\n",
    "            Pr    = tf.gather_nd(Pr   , zip_list)\n",
    "            Vthis = tf.gather_nd(Vthis, zip_list)\n",
    "            Vnext = tf.gather_nd(Vnext, zip_list)\n",
    "            \n",
    "            LogPr = tf.math.log(Pr) # pi(a|s)를 추적해 gradient를 씌워야 함\n",
    "            Delta = rewards + gamma * Vnext * (1 - done) - Vthis\n",
    "\n",
    "            ActorLoss  = - tf.math.reduce_sum(LogPr * Delta)    # Reinforce와 같은 원리\n",
    "            CriticLoss = tf.math.reduce_sum(Delta**2) # DeepMind 7강 참조. 결론부터 말하면 Loss는 RMS(v_pi(s) - v_w(s)) = RMS(G_t - v_w(s)) = RMS(delta)\n",
    "\n",
    "        gradActor  = tape.gradient(ActorLoss, self.policy.trainable_variables)\n",
    "        gradCritic = tape.gradient(CriticLoss, self.value.trainable_variables)\n",
    "        del tape    # 사용이 끝난 tape는 Garbage Collector에게 전달\n",
    "\n",
    "        self.policy.optimizer.apply_gradients(zip(gradActor, self.policy.trainable_variables))\n",
    "        self.value .optimizer.apply_gradients(zip(gradCritic, self.value.trainable_variables))\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.read_only:\n",
    "            raise Exception(\"ActorCriticDeploy class can only use model, not save.\")\n",
    "            return\n",
    "\n",
    "        self.value.save(self.filename + \"-value\")\n",
    "        self.policy.save(self.filename + \"-policy\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.value = keras.models.load_model(self.filename + \"-value\")\n",
    "        self.policy = keras.models.load_model(self.filename + \"-policy\")\n",
    "\n",
    "class ActorCriticDeploy(ActorCriticAgent):\n",
    "    ''' Model의 학습과 저장은 불가능하고 오직 사용만을 위한 class '''\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.read_only = True\n",
    "        self.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68P3OrbbE-h_"
   },
   "source": [
    "## Model Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG2Lsxp7E-iB"
   },
   "source": [
    "### Gym을 사용한 가상환경 준비\n",
    "OpenAI에서 제공하는 gym 모듈을 사용해 보자. 이 모듈은 `pip install gym`을 통해 설치할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_w-qeQaFE-iB"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 학습\n",
    "아래의 코드 블록을 실행하면 랜덤하게 초기화된 cartAgent가 강화학습을 시작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LmIROgcE-iC",
    "outputId": "672188ed-8adc-4518-a35e-2104d14fb570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.07014836  0.05053015]\n",
      " [-0.04496381  0.03167167]\n",
      " [-0.0944189   0.07007447]\n",
      " [-0.02010098  0.01429473]\n",
      " [-0.11688532  0.09004559]\n",
      " [-0.01882243  0.01534003]\n",
      " [-0.00068095  0.00263332]\n",
      " [-0.00223432  0.00272717]], shape=(8, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [4 1]\n",
      " [5 1]\n",
      " [6 1]\n",
      " [7 1]], shape=(8, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 0.05053015  0.03167167  0.07007447 -0.02010098  0.09004559  0.01534003\n",
      "  0.00263332  0.00272717], shape=(8,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13488/3094516267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# env.render() # 렉 걸릴 경우 제외\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mcartAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 점수 기록\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13488/1189890036.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mDelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mVnext\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mVthis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mActorLoss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogPr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mDelta\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Reinforce와 같은 원리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MLTensor\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MLTensor\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "cartAgent = ActorCriticAgent(\n",
    "    input_dims=env.observation_space.shape, \n",
    "    actor_rate=0.0003, \n",
    "    critic_rate=0.0003, \n",
    "    gamma=0.99,\n",
    "    n_actions=env.action_space.n, \n",
    "    replay_size=32,\n",
    "    batch_size=8,\n",
    "    filename='cartpole-ac-replay'\n",
    ")\n",
    "\n",
    "scores, scores_avg = list(), list()\n",
    "prgress = tqdm(range(1000))\n",
    "\n",
    "for episode in prgress:\n",
    "    # 환경 및 점수 초기화\n",
    "    score = 0\n",
    "    obs_this = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agent가 선택\n",
    "        action = cartAgent.choose_action(obs_this)\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        cartAgent.store(obs_this, action, reward, obs_next, done)\n",
    "\n",
    "        # 보상 저장\n",
    "        score += reward\n",
    "        obs_this = obs_next\n",
    "        # env.render() # 렉 걸릴 경우 제외\n",
    "\n",
    "        cartAgent.learn()\n",
    "\n",
    "    scores.append(score) # 점수 기록\n",
    "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
    "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f}\"\n",
    "        .format(scores[-1], scores_avg[-1]))\n",
    "\n",
    "    if episode % 200 == 0 and episode > 0: # 100 episode마다 모델을 저장\n",
    "        cartAgent.save_model()\n",
    "\n",
    "plt.plot(np.arange(len(scores_avg)), np.array(scores_avg)) # 이동평균 점수 그래프 그리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 학습한 모델을 불러와 실행하는 블록이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score 33.0 | recent average score 17.6: 100%|██████████| 5/5 [00:00<00:00,  7.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e134306d00>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cartDeploy = ActorCriticDeploy('cartpole-ac-replay')\n",
    "\n",
    "scores, scores_avg = list(), list()\n",
    "prgress = tqdm(range(5))\n",
    "\n",
    "for episode in prgress:\n",
    "    # 환경 및 점수 초기화\n",
    "    score = 0\n",
    "    obs_this = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agent가 선택\n",
    "        action = cartDeploy.choose_action(obs_this)\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "\n",
    "        # 보상 저장\n",
    "        score += reward\n",
    "        obs_this = obs_next\n",
    "        env.render() # 렉 걸릴 경우 제외\n",
    "\n",
    "    scores.append(score) # 점수 기록\n",
    "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
    "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f}\"\n",
    "        .format(scores[-1], scores_avg[-1]))\n",
    "\n",
    "plt.plot(np.arange(len(scores)), np.array(scores)) # 이동평균 점수 그래프 그리기"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe5c5529a470cf481d22924ed91de5c709ed4ff5a99c8b9a5a1fe3fe1062ceb2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MLTensor': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
