{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTTc1eEE-h2"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlcooZdcE-h5"
      },
      "source": [
        "### 패키지 import\n",
        "의존성 있는 패키지인 ``` numpy ```, ``` tensorflow ```, ``` keras ```를 import해야 한다.  \n",
        "  \n",
        "참고: keras는 단독으로 import하지 않고 반드시 ``` import tensorflow.keras as keras ```와 같이 ``` tensorflow ```를 통해 import해야 잠재적인 오류를 예방할 수 있다.  \n",
        "\n",
        "주어진 모듈을 설치하기 위해서는 `pip install numpy`, `pip install tensorflow`를 실행하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-hm_1nH0E-h6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hP_xapE-h7"
      },
      "source": [
        "### ReplayBuffer 클래스\n",
        "Replay buffer란 이전의 (S, a, S', R) 순서쌍들을 모아 둔 거대한 테이블을 뜻한다. 강화학습 특성상 환경과 상호작용하기에 시간 순서대로 데이터가 생성되는 즉시 학습시키면 bias가 발생하기 쉽다. 이러한 buffer를 도입함으로써 문제를 해결할 수 있다.  \n",
        " \n",
        "이 class는 다음과 같은 method로 구성된다.\n",
        "* 생성자\n",
        "  * ```replay_size``` 변수에 최대 버퍼크기 명시\n",
        "  * ```input_dims```에 state feature의 dimension 명시\n",
        "* ```store```을 통해 S, a, S', R 값과 terminate 여부를 저장\n",
        "* ```sample```을 통해 주어진 개수만큼 random sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "avPl1viwE-h7"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, replay_size, input_dims):\n",
        "        self.replay_size = replay_size\n",
        "        self.mem_counter = 0\n",
        "        \n",
        "        self.state = np.zeros((self.replay_size, *input_dims), dtype=np.float32)\n",
        "        self.next_state = np.zeros((self.replay_size, *input_dims), dtype=np.float32)\n",
        "        self.action = np.zeros(self.replay_size, dtype=np.int32)\n",
        "        self.reward = np.zeros(self.replay_size, dtype=np.float32)\n",
        "        self.terminate = np.zeros(self.replay_size, dtype=np.int32)\n",
        "    \n",
        "    def store(self, state, action, reward, next_state, terminate):\n",
        "        ''' ReplayBuffer에 단일 데이터쌍 저장 '''\n",
        "        index = self.mem_counter % self.replay_size\n",
        "\n",
        "        self.state[index]       = state\n",
        "        self.next_state[index]  = next_state\n",
        "        self.action[index]      = action\n",
        "        self.reward[index]      = reward\n",
        "        self.terminate[index]   = 1 - int(terminate)\n",
        "\n",
        "        self.mem_counter += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ''' ReplayBuffer에서 batch_size만큼 임의의 데이터 샘플링 '''\n",
        "\n",
        "        # 아직 replay buffer가 다 차지 않은 경우를 고려해 실제 space_size를 min함수를 이용해 산정\n",
        "        space_size = min(self.mem_counter, self.replay_size)\n",
        "        batch = np.random.choice(space_size, batch_size, replace=False)\n",
        "\n",
        "        state       = self.state[batch]\n",
        "        next_state  = self.next_state[batch]\n",
        "        action      = self.action[batch]\n",
        "        reward      = self.reward[batch]\n",
        "        terminate   = self.terminate[batch]\n",
        "\n",
        "        return state, action, reward, next_state, terminate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XrFwHf3E-h8"
      },
      "source": [
        "### ANN 모델 만들기\n",
        "DQN에서 Tabular action을 나타내는 model을 tensorflow를 통해 컴파일한다.\n",
        "* ``` l_rate ```는 학습률을 나타낸다.\n",
        "* ``` n_actions ```는 tabular action의 개수를 나타낸다.\n",
        "* ``` input_dims ```는 state의 feature dimension이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zJQuDKpEE-h9"
      },
      "outputs": [],
      "source": [
        "def build_dqn(l_rate, n_actions, input_dims):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Flatten(input_shape=input_dims),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(n_actions, activation=None)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=l_rate), loss='mse')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rR5DuztE-h9"
      },
      "source": [
        "아래는 간단한 ANN Model을 만드는 예제이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "04sVvSmOE-h-"
      },
      "outputs": [],
      "source": [
        "simple_ANN_model = build_dqn(1e-8, 100, (4,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZVCOCSnE-h-"
      },
      "source": [
        "### DQN Agent 만들기\n",
        "Agent는 위에서 만들어진 ` ReplayBuffer `와 ` build_dqn `을 함께 사용해 실제로 RL을 실행하는 클래스이다.\n",
        "* 생성자\n",
        "  * ` input_dims `는 입력 데이터의 차원이다.\n",
        "  * ` l_rate `는 학습률 $\\alpha$이다.\n",
        "  * ` gamma `는 미래할인률 $\\gamma$이다.\n",
        "  * ` n_actions `는 사용 가능한 action 수이다.\n",
        "  * ` batch_size `는 `ReplayBuffer`에서 한 번에 학습할 데이터 batch의 수이다.\n",
        "  * ` epsilon_start `, `epsilon_decay`, `epsilon_end`는 $\\epsilon$의 값을 통제한다. `start`에서 시작해 action을 선택할 때마다 `decay`만큼 깎이되, `end`보다 낮아지지 않는다.\n",
        "  * ` replay_size `는 `ReplayBuffer`의 크기를 결정한다.\n",
        "  * ` filename `은 모델을 저장할 파일의 이름이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jbWzc_CGE-h_"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, input_dims, l_rate, gamma, n_actions, batch_size,\n",
        "        epsilon_start, epsilon_decay, epsilon_end,\n",
        "        replay_size, filename):\n",
        "\n",
        "        self.read_only = False\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.action = [i for i in range(n_actions)]\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.eps = epsilon_start\n",
        "        self.eps_decay = epsilon_decay\n",
        "        self.eps_end = epsilon_end\n",
        "\n",
        "        self.memory = ReplayBuffer(replay_size, input_dims)\n",
        "        self.Q = build_dqn(l_rate, n_actions, input_dims)\n",
        "\n",
        "        self.filename = filename\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminate):\n",
        "        ''' internal ReplayBuffer에 상태 저장 '''\n",
        "        if self.read_only:\n",
        "            raise Exception(\"DQNDeploy class can only use model, not store.\")\n",
        "            return\n",
        "        self.memory.store(state, action, reward, next_state, terminate)\n",
        "\n",
        "    def choose_action(self, obs):\n",
        "        ''' observation `obs`를 토대로 epsilon-greddy를 따르는 action 결정 '''\n",
        "        if np.random.random() < self.eps:\n",
        "            # epsilon하게 exploration\n",
        "            action = np.random.choice(self.action)\n",
        "        else:\n",
        "            # argmax Q 선택\n",
        "            state = np.array([obs])\n",
        "            action_set = self.Q.predict(state)\n",
        "            action = np.argmax(action_set)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        ''' 쌓아 둔 데이터로 random batch한 뒤 강화학습 진행 '''\n",
        "        if self.read_only:\n",
        "            raise Exception(\"DQNDeploy class can only use model, not learn.\")\n",
        "            return\n",
        "\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # n x input, n x 1, n x 1, n x input, n x 1\n",
        "        states, actions, rewards, next_states, terminates = \\\n",
        "            self.memory.sample(self.batch_size)\n",
        "\n",
        "        # n x n\n",
        "        q_this = self.Q.predict(states)\n",
        "        q_next = self.Q.predict(next_states)\n",
        "\n",
        "        q_target = np.copy(q_this)\n",
        "        enum_list = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        # 과거 시행되었던 state-action pair에 대하여 bellman equation 업데이트\n",
        "        q_target[enum_list, actions.astype(np.int32)] = rewards + \\\n",
        "            self.gamma * np.max(q_next, axis=1) * terminates\n",
        "\n",
        "        # n x input ~ n x 1 에 대해 train on batch\n",
        "        self.Q.train_on_batch(states, q_target)\n",
        "        \n",
        "        # epsilon decay\n",
        "        if self.eps > self.eps_end:\n",
        "            self.eps -= self.eps_decay\n",
        "            \n",
        "    def save_model(self):\n",
        "        if self.read_only:\n",
        "            raise Exception(\"DQNDeploy class can only use model, not save.\")\n",
        "            return\n",
        "\n",
        "        self.Q.save(self.filename)\n",
        "    \n",
        "    def load_model(self):\n",
        "        self.Q = keras.models.load_model(self.filename)\n",
        "\n",
        "class DQNDeploy(DQNAgent):\n",
        "    ''' Model의 학습과 저장은 불가능하고 오직 사용만을 위한 class '''\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.eps = 0\n",
        "        self.read_only = True\n",
        "        self.load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68P3OrbbE-h_"
      },
      "source": [
        "## Model Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG2Lsxp7E-iB"
      },
      "source": [
        "### Gym을 사용한 가상환경 준비\n",
        "OpenAI에서 제공하는 gym 모듈을 사용해 보자. 이 모듈은 `pip install gym`을 통해 설치할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_w-qeQaFE-iB"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "observation = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVg78fIQE-iB"
      },
      "source": [
        "이 상황에서 `DQNAgent`를 도입해 문제를 해결해 보자. 이때 observation은 `env.observation_space.shape`의 dimensionality를, action은 `env.action_space.n`의 dimensionality를 가진다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHEAFFUeE-iB",
        "outputId": "607a5eb7-dce7-4981-c2b9-6b713dab8345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space.shape)\n",
        "print(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 학습\n",
        "아래의 코드 블록을 실행하면 랜덤하게 초기화된 cartAgent가 강화학습을 시작한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LmIROgcE-iC",
        "outputId": "672188ed-8adc-4518-a35e-2104d14fb570"
      },
      "outputs": [],
      "source": [
        "cartAgent = DQNAgent(\n",
        "    input_dims=env.observation_space.shape, \n",
        "    l_rate=0.001, \n",
        "    gamma=0.98,\n",
        "    n_actions=env.action_space.n, \n",
        "    batch_size=64, \n",
        "    epsilon_start=1, \n",
        "    epsilon_decay=0.005,\n",
        "    epsilon_end=0.01,\n",
        "    replay_size=10000,\n",
        "    filename='cartpole-dqn'\n",
        ")\n",
        "\n",
        "scores, scores_avg = list(), list()\n",
        "prgress = tqdm(range(500))\n",
        "\n",
        "for episode in prgress:\n",
        "    # 환경 및 점수 초기화\n",
        "    score = 0\n",
        "    obs_this = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Agent가 선택\n",
        "        action = cartAgent.choose_action(obs_this)\n",
        "        obs_next, reward, done, info = env.step(action)\n",
        "        cartAgent.store(obs_this, action, reward, obs_next, done)\n",
        "\n",
        "        # 보상 저장\n",
        "        score += reward\n",
        "        obs_this = obs_next\n",
        "        # env.render() # 렉 걸릴 경우 제외\n",
        "        \n",
        "        # Agent 학습\n",
        "        cartAgent.learn()\n",
        "\n",
        "    scores.append(score) # 점수 기록\n",
        "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
        "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f} | epsilon {:>.2f}\"\n",
        "        .format(scores[-1], scores_avg[-1], cartAgent.eps))\n",
        "\n",
        "    if episode % 50 == 0: # 50 episode마다 모델을 저장\n",
        "        cartAgent.save_model()\n",
        "\n",
        "plt.plot(np.arange(len(scores_avg)), np.array(scores_avg)) # 이동평균 점수 그래프 그리기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음은 학습한 모델을 불러와 실행하는 블록이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score 500.0 | recent average score 394.0 | epsilon 0.00: 100%|██████████| 5/5 [01:17<00:00, 15.47s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1dfb32883d0>]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cartDeploy = DQNDeploy('cartpole-dqn')\n",
        "\n",
        "scores, scores_avg = list(), list()\n",
        "prgress = tqdm(range(5))\n",
        "\n",
        "for episode in prgress:\n",
        "    # 환경 및 점수 초기화\n",
        "    score = 0\n",
        "    obs_this = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Agent가 선택\n",
        "        action = cartDeploy.choose_action(obs_this)\n",
        "        obs_next, reward, done, info = env.step(action)\n",
        "\n",
        "        # 보상 저장\n",
        "        score += reward\n",
        "        obs_this = obs_next\n",
        "        env.render() # 렉 걸릴 경우 제외\n",
        "\n",
        "    scores.append(score) # 점수 기록\n",
        "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
        "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f} | epsilon {:>.2f}\"\n",
        "        .format(scores[-1], scores_avg[-1], cartDeploy.eps))\n",
        "\n",
        "plt.plot(np.arange(len(scores)), np.array(scores)) # 이동평균 점수 그래프 그리기"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DQN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "fe5c5529a470cf481d22924ed91de5c709ed4ff5a99c8b9a5a1fe3fe1062ceb2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('MLTensor': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
