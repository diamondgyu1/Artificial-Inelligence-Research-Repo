{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE에서와 같이 **episodic reward**를 reward function으로 설정하자. 정책 $\\pi_\\theta$에서 reward function $J(\\theta)$는 다음과 같이 정의된다.\n",
    "\n",
    "$$ J(\\theta) = \\mathbb{E}_{S_0 \\sim d_0, \\pi_\\theta}\\left[ \\sum_{t=0}^{\\infty}{\\gamma^t R_{t+1}} \\right] = \\mathbb{E}_{S_0 \\sim d_0, \\pi_\\theta}\\left[ G_0 \\right] = \\mathbb{E}_{S_0 \\sim d_0}\\left[ v_{\\pi_\\theta} (S_0) \\right] $$\n",
    "\n",
    "Policy Gradient Theorem(episodic)에 의하여, 위 식의 그래디언트인 $\\nabla_\\theta J(\\theta)$ 는 다음과 같다.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\left. \\sum_{t=0}^{T}{\\gamma^t q_{\\pi_\\theta}(S_t, A_t) \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t)} \\right| S_0 \\sim d_0 \\right]$$\n",
    "\n",
    "또한, 수학적으로 다음과 같은 성질이 성립한다.\n",
    "\n",
    "$$ \\mathbb{E}_{\\pi_\\theta} \\left[ b \\cdot \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t) \\right] = 0 $$\n",
    "\n",
    "이 성질을 사용하면 단순히 $q_{\\pi_\\theta}(S_t, A_t)$ 를 reward로 사용하는 것이 아닌, 평균을 빼서 더 안정적인 reward를 계산할 수 있다. 그 평균값은 $v_{\\pi_\\theta}(S_t)$ 이며, 이를 반영한 gradient 식은 다음과 같다.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\left. \\sum_{t=0}^{T}{\\gamma^t (q_{\\pi_\\theta}(S_t, A_t) - v_{\\pi_\\theta}(S_t) ) \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t)} \\right| S_0 \\sim d_0 \\right]$$\n",
    "\n",
    "$q_{\\pi_\\theta}(S_t, A_t) = \\mathbb{E}_{\\pi_\\theta}\\left[ \\left. G_t \\right| S_t, A_t \\right]$ 이므로 지금과 같은 stochastic 조건 하에서 $q_{\\pi_\\theta}(S_t, A_t) \\approx G_t$ 로 간주할 수 있다. 다만 이 경우에서는 $v_{\\pi_\\theta}$ 함수를 사용할 수 있으므로 각 시행마다 발생하는 variance를 줄이기 위해 $G_t$ 대신 $G_t \\approx R_{t+1} + \\gamma v_{\\textbf{w}}(S_{t+1})$ 를 사용한다. 또한 일반적으로 $\\gamma^t$ 항은 생략한다.\n",
    "\n",
    "기존의 Monte-Carlo식의 REINFORCE와는 다르게, Actor-Critic에서 업데이트는 매 $t$ 마다 아래처럼 이루어진다.\n",
    "\n",
    "$$ \\delta_t = R_{t+1} + \\gamma v_{\\textbf{w}}(S_{t+1}) - v_{\\textbf{w}}(S_t) $$\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\beta \\cdot \\delta_t \\nabla_\\textbf{w} v_{\\textbf{w}}(S_t) $$\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha \\cdot \\delta_t \\nabla_\\theta \\log \\pi_\\theta (A_t \\mid S_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlcooZdcE-h5"
   },
   "source": [
    "### 패키지 import\n",
    "의존성 있는 패키지인 ``` numpy ```, ``` tensorflow ```, ``` keras ```를 import해야 한다.  \n",
    "  \n",
    "참고: keras는 단독으로 import하지 않고 반드시 ``` import tensorflow.keras as keras ```와 같이 ``` tensorflow ```를 통해 import해야 잠재적인 오류를 예방할 수 있다.  \n",
    "\n",
    "주어진 모듈을 설치하기 위해서는 `pip install numpy`, `pip install tensorflow`를 실행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-hm_1nH0E-h6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor & Critic Model Building\n",
    "* ``` l_rate ```는 학습률을 나타낸다.\n",
    "* ``` n_actions ```는 tabular action의 개수를 나타낸다.\n",
    "* ``` input_dims ```는 state의 feature dimension이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    # pi 함수\n",
    "    def __init__(self, l_rate, n_actions, input_dims):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.l_rate     = l_rate\n",
    "        self.n_actions  = n_actions\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.layer1 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer2 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer3 = keras.layers.Dense(n_actions, activation='softmax')\n",
    "\n",
    "        self.compile(optimizer=keras.optimizers.Adam(learning_rate=self.l_rate))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mid1 = self.layer1(inputs)\n",
    "        mid2 = self.layer2(mid1)\n",
    "        return self.layer3(mid2)\n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    # v 함수\n",
    "    def __init__(self, l_rate, n_actions, input_dims):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.l_rate     = l_rate\n",
    "        self.n_actions  = n_actions\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.layer1 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer2 = keras.layers.Dense(64, activation='relu')\n",
    "        self.layer3 = keras.layers.Dense(n_actions)\n",
    "\n",
    "        self.compile(optimizer=keras.optimizers.Adam(learning_rate=self.l_rate))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mid1 = self.layer1(inputs)\n",
    "        mid2 = self.layer2(mid1)\n",
    "        return self.layer3(mid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoricalSelect(L):\n",
    "    ''' L[i] 확률로 i를 반환하는 함수. 어떠한 i에도 해당하지 않으면 0 반환. 0 <= sum(L) <= 1인 경우에만 정의됨.'''\n",
    "    L = list(tf.squeeze(L))\n",
    "    pick = np.random.random()\n",
    "    cumul = 0\n",
    "\n",
    "    for i, pi in enumerate(L):\n",
    "        if (cumul <= pick < cumul + float(pi)):\n",
    "            return i\n",
    "        cumul += pi\n",
    "\n",
    "    return 0\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, input_dims, actor_rate, critic_rate, gamma, n_actions, filename):\n",
    "        self.read_only = False\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.action = [i for i in range(n_actions)]\n",
    "\n",
    "        self.policy = ActorNetwork(actor_rate, n_actions, input_dims)\n",
    "        self.value  = CriticNetwork(critic_rate, n_actions, input_dims)\n",
    "\n",
    "        # model 저장용\n",
    "        self.filename = filename\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        ''' 내부 stochastic policy인 `self.policy`를 토대로 action 결정 '''\n",
    "        state = np.array([obs], dtype=np.float32) # 관찰 결과를 numpy 스타일로 변환 (tf.convert_to_tensor도 ok)\n",
    "        pr = self.policy(state) # 신경망으로 전달, 확률 pr에 저장\n",
    "        return categoricalSelect(pr)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        if self.read_only:\n",
    "            raise Exception(\"ActorCriticDeploy class can only use model, not learn.\")\n",
    "            return\n",
    "        \n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        next_state = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:  # Gradient를 policy/value에 2회 적용하므로 persistent\n",
    "            Pr    = tf.squeeze(self.policy(state))\n",
    "            Vthis = tf.squeeze(self.value(state))\n",
    "            Vnext = tf.squeeze(self.value(next_state))\n",
    "\n",
    "            LogPr = tf.math.log(Pr[action]) # pi(a|s)를 추적해 gradient를 씌워야 함\n",
    "            Delta = reward + self.gamma * Vnext * (1 - int(done)) - Vthis\n",
    "\n",
    "            ActorLoss  = - LogPr * Delta    # Reinforce와 같은 원리\n",
    "            CriticLoss = Delta ** 2 # DeepMind 7강 참조. 결론부터 말하면 Loss는 RMS(v_pi(s) - v_w(s)) = RMS(G_t - v_w(s)) = RMS(delta)\n",
    "\n",
    "        gradActor  = tape.gradient(ActorLoss, self.policy.trainable_variables)\n",
    "        gradCritic = tape.gradient(CriticLoss, self.value.trainable_variables)\n",
    "        del tape    # 사용이 끝난 tape는 GarbageCollector에게 전달\n",
    "\n",
    "        self.policy.optimizer.apply_gradients(zip(gradActor, self.policy.trainable_variables))\n",
    "        self.value .optimizer.apply_gradients(zip(gradCritic, self.value.trainable_variables))\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.read_only:\n",
    "            raise Exception(\"ActorCriticDeploy class can only use model, not save.\")\n",
    "            return\n",
    "\n",
    "        self.value.save(self.filename + \"-value\")\n",
    "        self.policy.save(self.filename + \"-policy\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.value = keras.models.load_model(self.filename + \"-value\")\n",
    "        self.policy = keras.models.load_model(self.filename + \"-policy\")\n",
    "\n",
    "class ActorCriticDeploy(ActorCriticAgent):\n",
    "    ''' Model의 학습과 저장은 불가능하고 오직 사용만을 위한 class '''\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.read_only = True\n",
    "        self.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68P3OrbbE-h_"
   },
   "source": [
    "## Model Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG2Lsxp7E-iB"
   },
   "source": [
    "### Gym을 사용한 가상환경 준비\n",
    "OpenAI에서 제공하는 gym 모듈을 사용해 보자. 이 모듈은 `pip install gym`을 통해 설치할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_w-qeQaFE-iB"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 학습\n",
    "아래의 코드 블록을 실행하면 랜덤하게 초기화된 cartAgent가 강화학습을 시작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LmIROgcE-iC",
    "outputId": "672188ed-8adc-4518-a35e-2104d14fb570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score 26.0 | recent average score 72.8: 100%|██████████| 200/200 [02:51<00:00,  1.17it/s]  \n"
     ]
    }
   ],
   "source": [
    "cartAgent = ActorCriticAgent(\n",
    "    input_dims=env.observation_space.shape, \n",
    "    actor_rate=0.003, \n",
    "    critic_rate=0.003, \n",
    "    gamma=0.98,\n",
    "    n_actions=env.action_space.n, \n",
    "    filename='cartpole-ac'\n",
    ")\n",
    "\n",
    "scores, scores_avg = list(), list()\n",
    "prgress = tqdm(range(200))\n",
    "\n",
    "for episode in prgress:\n",
    "    # 환경 및 점수 초기화\n",
    "    score = 0\n",
    "    obs_this = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agent가 선택\n",
    "        action = cartAgent.choose_action(obs_this)\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        cartAgent.learn(obs_this, action, reward, obs_next, done)\n",
    "\n",
    "        # 보상 저장\n",
    "        score += reward\n",
    "        obs_this = obs_next\n",
    "        # env.render() # 렉 걸릴 경우 제외\n",
    "\n",
    "    scores.append(score) # 점수 기록\n",
    "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
    "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f}\"\n",
    "        .format(scores[-1], scores_avg[-1]))\n",
    "\n",
    "    if episode % 200 == 0 and episode > 0: # 100 episode마다 모델을 저장\n",
    "        cartAgent.save_model()\n",
    "\n",
    "plt.plot(np.arange(len(scores_avg)), np.array(scores_avg)) # 이동평균 점수 그래프 그리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 학습한 모델을 불러와 실행하는 블록이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartDeploy = ActorCriticDeploy('cartpole-ac')\n",
    "\n",
    "scores, scores_avg = list(), list()\n",
    "prgress = tqdm(range(5))\n",
    "\n",
    "for episode in prgress:\n",
    "    # 환경 및 점수 초기화\n",
    "    score = 0\n",
    "    obs_this = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agent가 선택\n",
    "        action = cartDeploy.choose_action(obs_this)\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "\n",
    "        # 보상 저장\n",
    "        score += reward\n",
    "        obs_this = obs_next\n",
    "        env.render() # 렉 걸릴 경우 제외\n",
    "\n",
    "    scores.append(score) # 점수 기록\n",
    "    scores_avg.append(np.mean(scores[-30:])) # 이동평균 산출\n",
    "    prgress.set_description(\"score {:>3.1f} | recent average score {:>3.1f} | epsilon {:>.2f}\"\n",
    "        .format(scores[-1], scores_avg[-1], cartDeploy.eps))\n",
    "\n",
    "plt.plot(np.arange(len(scores)), np.array(scores)) # 이동평균 점수 그래프 그리기"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe5c5529a470cf481d22924ed91de5c709ed4ff5a99c8b9a5a1fe3fe1062ceb2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MLTensor': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
